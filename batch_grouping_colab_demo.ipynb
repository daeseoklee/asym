{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batch_grouping_colab_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Grouping using asym"
      ],
      "metadata": {
        "id": "2zxCGwJtnIBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have a GPU runtime"
      ],
      "metadata": {
        "id": "LZx2lEdfm51C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVdvfbgIUWeo",
        "outputId": "2f2c8f9c-865d-4556-aecc-7fad52d7be2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar  9 10:56:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing asym"
      ],
      "metadata": {
        "id": "1-leK17OnT-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/daeseoklee/asym\n",
        "! mv asym tmp \n",
        "! mv tmp/asym asym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY94xMPGYJ_i",
        "outputId": "174ba704-c091-46a8-a81f-fb09e2464a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'asym' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import choice\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import pad\n",
        "from torch.utils.data import DataLoader\n",
        "from asym.annotated_module import AnnotatedModule\n",
        "from asym.data_collection import DataCollection\n",
        "from asym.grouper import LengthThresholdGrouper, UniGrouper\n",
        "from asym.precompute_grouper_thresholds import find_thresholds\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nzBt5-5GUqXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset\n",
        "\n",
        "Let's create a peculiar virtual dataset, where \n",
        "* Datapoints have input length in {1, 4, 16, 64}\n",
        "* Each length appears with the frequency that is inversely proportional to the value\n",
        "* Each input is associated with a binary label"
      ],
      "metadata": {
        "id": "FBxWjW6BZKvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = 1024\n",
        "\n",
        "length_distribution = {4 ** i: 4 ** (-i) for i in range(4)}\n",
        "total_weight = sum(length_distribution.values())\n",
        "length_distribution = {num: weight / total_weight for num, weight in length_distribution.items()}\n",
        "lengths = choice(list(length_distribution.keys()), size=dataset_size, replace=True, p=list(length_distribution.values())).tolist()\n",
        "\n",
        "dataset = [(torch.randint(0, 8, (length,)), choice([0, 1])) for length in lengths]\n",
        "\n",
        "print(f'{dataset_size} datapoints')\n",
        "count = 0 \n",
        "for i, data in enumerate(dataset):\n",
        "  print(f'{i}-th datapoint:')\n",
        "  print(f'\\t{data}')\n",
        "  if len(data[0]) >= 2:\n",
        "    count += 1 \n",
        "    if count == 2:\n",
        "      break\n",
        "print('.\\n.\\n.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4JivUwOgJiD",
        "outputId": "2ba68867-d430-4ce0-e729-7e32e195b4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024 datapoints\n",
            "0-th datapoint:\n",
            "\t(tensor([3]), 0)\n",
            "1-th datapoint:\n",
            "\t(tensor([3]), 0)\n",
            "2-th datapoint:\n",
            "\t(tensor([2]), 0)\n",
            "3-th datapoint:\n",
            "\t(tensor([0, 5, 4, 0]), 1)\n",
            "4-th datapoint:\n",
            "\t(tensor([2]), 0)\n",
            "5-th datapoint:\n",
            "\t(tensor([3, 5, 4, 3]), 0)\n",
            ".\n",
            ".\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard way of training"
      ],
      "metadata": {
        "id": "Nsb9peCbugJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model consists of embedding -> linear map -> max pooling (ignoring padded positions) -> final linear map "
      ],
      "metadata": {
        "id": "EC6ajlZMvw4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AModule(nn.Module):\n",
        "  def __init__(self, emb_dim=1024, hidden_dim=1024, device='cuda:0'):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(8, emb_dim, device=device) #8 token types, dimension: emb_dim\n",
        "    self.linear = nn.Linear(emb_dim, hidden_dim, device=device)\n",
        "    self.proj = nn.Linear(hidden_dim, 1, device=device)\n",
        "  def forward(self, d):\n",
        "    x = d['x']\n",
        "    mask = d['mask']\n",
        "    assert x.shape == mask.shape \n",
        "    assert len(x.shape) == 2\n",
        "    x = self.emb(x)\n",
        "    x = self.linear(x) #(batch, length, )\n",
        "    minus_inf = torch.tensor(-float('inf'), device=x.device)\n",
        "    x = torch.where(mask[:, :, None].broadcast_to(x.shape), x, minus_inf) #Ignoring meaningless padded values through the next torch.max application.  \n",
        "    x = torch.max(x, dim=1).values\n",
        "    return self.proj(x)[:, 0]"
      ],
      "metadata": {
        "id": "NSCFoiw2cMON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the standard padding strategy, while remembering the padded positions."
      ],
      "metadata": {
        "id": "Wi45HNmlwNWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_collate_fn(data_list, target_device='cuda:0'):\n",
        "  xs = [x.to(device=target_device) for x, _ in data_list]\n",
        "  ys = [y for _, y in data_list]\n",
        "  max_len = max(len(x) for x in xs)\n",
        "  batch = {\n",
        "      'x': torch.stack([pad(x, (0, max_len - len(x))) for x in xs], dim=0),\n",
        "      'mask': torch.stack([pad(torch.ones_like(x, dtype=torch.bool), (0, max_len - len(x))) for x in xs], dim=0)\n",
        "  }\n",
        "  y = torch.tensor(ys, dtype=torch.float32, device=target_device)\n",
        "  return batch, y"
      ],
      "metadata": {
        "id": "VTH9Zl3mwtMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usual training proceduere:"
      ],
      "metadata": {
        "id": "7ZMON0Z4Jeqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(dataset, batch_size=256, collate_fn=lambda batch: standard_collate_fn(batch, target_device='cuda:0'))\n",
        "model = AModule(emb_dim=8192, hidden_dim=8192, device='cuda:0')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "def train_standard():\n",
        "  optimizer.zero_grad()\n",
        "  for i, (batch, y) in enumerate(tqdm(loader)):\n",
        "    out = model(batch)\n",
        "    mse = torch.mean((y - out) ** 2)\n",
        "    mse.backward()\n",
        "    optimizer.step() \n",
        "    optimizer.zero_grad()\n",
        "%time train_standard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5w9bdzX2YiM",
        "outputId": "7ef49177-7706-479d-f5a4-d7ad7394b668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:03<00:00,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.28 s, sys: 7.48 ms, total: 3.29 s\n",
            "Wall time: 3.29 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi | grep MiB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK4Tn4kA3-UL",
        "outputId": "6ea7f55e-788d-4c42-c3d5-97a6a61ee3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| N/A   57C    P0    47W / 250W |   4353MiB / 16280MiB |    100%      Default |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check \n",
        "1. The time spent in training\n",
        "2. The maximum GPU usage so far. "
      ],
      "metadata": {
        "id": "obVjhLO65DfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with Batch Grouping"
      ],
      "metadata": {
        "id": "NY3hU4bIvKhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Length threshold values are computed here from the global dataset-level statistics via a heuristic algorithm. The \"cost_fn\" is linear, since the preceding `AModule`'s memory usage would be linearly proportional to the length of the batch (which is the maximum sequence length of the pre-batch datapoints)."
      ],
      "metadata": {
        "id": "vzvRzydb8vFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(data[0]) for data in dataset]\n",
        "length_thresholds = find_thresholds(lengths, cost_fn=lambda x: x, k=3, num_trials=100)\n",
        "print('thresholds:', length_thresholds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-FGsIdxvTmR",
        "outputId": "540ce0bc-0743-48da-dd26-0bf98bf91061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thresholds: [1, 4, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to slightly modify the AModule class definition, to use it in asym \n",
        "1. Inherit from `AnnotatedModule` instead of `nn.Module`\n",
        "2. Specify the input and output shapes. ('b' stand for batch dimension, and 'l' stands for length dimension)\n",
        "3. Specify the form of the mask tensor, if you want to pass it to a `forward()` argument. Here, `['seq']` means you want the mask tensor to be of the form `'(b, l_seq)'`. "
      ],
      "metadata": {
        "id": "n8bgNv7wDQuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnnotatedAModule(AnnotatedModule):\n",
        "  def __init__(self, emb_dim=1024, hidden_dim=1024, device='cuda:0'):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(8, emb_dim, device=device) #8 token types, dimension: emb_dim\n",
        "    self.linear = nn.Linear(emb_dim, hidden_dim, device=device)\n",
        "    self.proj = nn.Linear(hidden_dim, 1, device=device)\n",
        "  def forward(self, x, mask):\n",
        "    assert x.shape == mask.shape \n",
        "    assert len(x.shape) == 2\n",
        "    x = self.emb(x)\n",
        "    x = self.linear(x) #(batch, length, )\n",
        "    minus_inf = torch.tensor(-float('inf'), device=x.device)\n",
        "    x = torch.where(mask[:, :, None].broadcast_to(x.shape), x, minus_inf) #Ignoring meaningless padded values through the next torch.max application.  \n",
        "    x = torch.max(x, dim=1).values\n",
        "    return self.proj(x)[:, 0]\n",
        "  def get_mask_hint(self):\n",
        "    return ['seq']\n",
        "  def get_input_annot(self):\n",
        "    return '(b, l_seq)'\n",
        "  def get_output_annot(self):\n",
        "    return '(b)'\n"
      ],
      "metadata": {
        "id": "hJ1u_5i1-9F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's our new collate_fn. `(B, L_sequence)` specifies the shape of the data. The label `'sequence'` will be recognized by `LengthThresholdGrouper` later on."
      ],
      "metadata": {
        "id": "kwQxkJ6zMk01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_collate_fn(data_list, target_device='cuda:0'):\n",
        "  xs = [x.to(device=target_device) for x, _ in data_list]\n",
        "  ys = [y for _, y in data_list]\n",
        "  dc = DataCollection('(B, L_sequence)', xs)\n",
        "  y = torch.tensor(ys, dtype=torch.float32, device=target_device)\n",
        "  return dc, y "
      ],
      "metadata": {
        "id": "-pwZGaG4LtFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified training procedure:"
      ],
      "metadata": {
        "id": "C9UL6j6lNIoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_loader = DataLoader(dataset, batch_size=256, collate_fn=lambda batch: new_collate_fn(batch, target_device='cuda:0'))\n",
        "new_model = AnnotatedAModule(emb_dim=8192, hidden_dim=8192, device='cuda:0')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "def train_new():\n",
        "  optimizer.zero_grad()\n",
        "  for i, (batch, y) in enumerate(tqdm(new_loader)):\n",
        "    grouper = LengthThresholdGrouper('sequence', length_thresholds) #batch grouping \n",
        "    batch.group(grouper=grouper)\n",
        "    out = batch.apply(new_model) \n",
        "    out.regroup(grouper=UniGrouper()) #forming the usual minibatch of outputs\n",
        "    out = out.data_groups[0].value\n",
        "    mse = torch.mean((y - out) ** 2) \n",
        "    mse.backward()\n",
        "    optimizer.step() \n",
        "    optimizer.zero_grad()\n",
        "%time train_new()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_7eMalBNLIX",
        "outputId": "e690aef0-f8c7-48fd-ab11-10ad46993ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  7.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 560 ms, sys: 2.42 ms, total: 562 ms\n",
            "Wall time: 567 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi | grep MiB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFuPmu5AP2ds",
        "outputId": "6a86bca3-24fb-444f-ce24-6c9a2f8e1c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| N/A   56C    P0    46W / 250W |   4353MiB / 16280MiB |     46%      Default |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the improved efficiency. "
      ],
      "metadata": {
        "id": "dSi-jHeTQodX"
      }
    }
  ]
}